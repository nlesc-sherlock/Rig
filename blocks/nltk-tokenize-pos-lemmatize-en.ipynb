{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK tokenize pos lemmatize (en)\n",
    "\n",
    "Code block that tokenizes, part-of-speech tags, and lemmatizes a text field in the input data frame, using NLTK tools. First sentences are tokenized using the English sentence tokenizer. Next, words are tokenized using the TreebankWordTokenizer. \n",
    "\n",
    "The data frame that is returned, contains fields:\n",
    "\n",
    "* `token` (`str`): the word extracted by the tokenizer\n",
    "* `tokennumber` (`int`): the index of the token in the sentence\n",
    "* `sentencenumber` (`int`): the index of the sentence in the text\n",
    "* `textid` (`str`): the text id\n",
    "* `pos` (`str`): the part of speech tag of the word extracted by the tokenizer\n",
    "* `lemma` (`str`): the lemma of the word extracted by the tokenizer\n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "Row(token='Dear', tokennumber=0, sentencenumber=0, textid='text1', pos='NNP', lemma='Dear')\n",
    "Row(token='phillip', tokennumber=1, sentencenumber=0, textid='text1', pos='NN', lemma='phillip')\n",
    "Row(token=',', tokennumber=2, sentencenumber=0, textid='text1', pos=',', lemma=',')\n",
    "```\n",
    "\n",
    "This block requires `nltk` and `nltk.data` to be installed.\n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "python -m nltk.downloader all\n",
    "```\n",
    "\n",
    "**Please note**: This block requires python 3 (because somehow, nltk is only installed under python 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.3.0 pyspark-shell'\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('text1', \"\"\"Dear phillip,\n",
    "\n",
    "\n",
    "This e-mail is automated notification of the availability of your\n",
    "current Natural Gas Intelligence Newsletter(s). Please use your\n",
    "username of \"pallen\" and your password to access\n",
    "\n",
    "\n",
    "       NGI's Daily Gas Price Index\n",
    "\n",
    "\n",
    "  http://intelligencepress.com/subscribers/index.html\n",
    "\n",
    "If you have forgotten your password please visit\n",
    "  http://intelligencepress.com/password.html\n",
    "and we will send it to you.\n",
    "\n",
    "If you would like to stop receiving e-mail notifications when your\n",
    "publications are available, please reply to this message with\n",
    "REMOVE E-MAIL in the subject line.\n",
    "\n",
    "Thank you for your subscription.\n",
    "\n",
    "For information about Intelligence Press products and services,\n",
    "visit our web site at http://intelligencepress.com or\n",
    "call toll-free (800) 427-5747.\n",
    "\n",
    "ALL RIGHTS RESERVED. (c) 2000, Intelligence Press, Inc.\n",
    "---\"\"\"),\n",
    "           ('text2', \"\"\"George,\n",
    "\n",
    " I received the drawings.  They look good at first glance.  I will look at \n",
    "them in depth this weekend.  The proforma was in the winmail.dat format which \n",
    "I cannot open.  Please resend in excel or a pdf format.  If you will send it \n",
    "to pallen70@hotmail.com, I will be able to look at it this weekend.  Does \n",
    "this file have a timeline for the investment dollars?  I just want to get a \n",
    "feel for when you will start needing money.  \n",
    "\n",
    " \n",
    "Phillip\"\"\")]\n",
    "data_in = sqlContext.createDataFrame(data)\n",
    "data_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments\n",
    "\n",
    "* `textid_field_index` (`int`): the index of the textid field in the row of the input data frame\n",
    "* `text_field_index` (`int`): the index of the text field in the row of the input data frame, this field contains the text that should be tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textid_field_index = 0\n",
    "text_field_index = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "arguments": [
     "textid_field_index",
     "text_field_index"
    ],
    "blockname": "nltk-tokenize-en",
    "collapsed": false,
    "input": "data_in",
    "output": "data_out",
    "type": "block"
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "fields = ('token', 'tokennumber', 'sentencenumber', 'textid', 'pos', 'lemma')\n",
    "Token = namedtuple('Token', fields)\n",
    "\n",
    "# convert penn treebank pos tags to wordnet pos tags\n",
    "# source: http://stackoverflow.com/questions/25534214/nltk-wordnet-lemmatizer-shouldnt-it-lemmatize-all-inflections-of-a-word\n",
    "wnpos = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "\n",
    "def tokenize_text(row):\n",
    "    text = row[text_field_index]\n",
    "    textid = row[textid_field_index]\n",
    "  \n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    word_tokenizer = TreebankWordTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for i, s in enumerate(sent_tokenizer.tokenize(text)):\n",
    "        sentence_pos = nltk.pos_tag(word_tokenizer.tokenize(s))\n",
    "        for j, (token, pos) in enumerate(sentence_pos):\n",
    "            lemma = lemmatizer.lemmatize(token, pos=wnpos(pos))\n",
    "            tokens.append(Token(token, j, i, textid, pos, lemma))\n",
    "    return tokens\n",
    "\n",
    "data_out = data_in.flatMap(tokenize_text).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n",
      "[Row(token='Dear', tokennumber=0, sentencenumber=0, textid='text1', pos='NNP', lemma='Dear'), Row(token='phillip', tokennumber=1, sentencenumber=0, textid='text1', pos='NN', lemma='phillip'), Row(token=',', tokennumber=2, sentencenumber=0, textid='text1', pos=',', lemma=','), Row(token='This', tokennumber=3, sentencenumber=0, textid='text1', pos='DT', lemma='This'), Row(token='e-mail', tokennumber=4, sentencenumber=0, textid='text1', pos='NN', lemma='e-mail'), Row(token='is', tokennumber=5, sentencenumber=0, textid='text1', pos='VBZ', lemma='be'), Row(token='automated', tokennumber=6, sentencenumber=0, textid='text1', pos='VBN', lemma='automate'), Row(token='notification', tokennumber=7, sentencenumber=0, textid='text1', pos='NN', lemma='notification'), Row(token='of', tokennumber=8, sentencenumber=0, textid='text1', pos='IN', lemma='of'), Row(token='the', tokennumber=9, sentencenumber=0, textid='text1', pos='DT', lemma='the')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[token: string, tokennumber: bigint, sentencenumber: bigint, textid: string, pos: string, lemma: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_out.count())\n",
    "print(data_out.take(10))\n",
    "data_out"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
